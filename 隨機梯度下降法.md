## 隨機梯度下降法 - 實作

### 生成一個Data (和最小平方法的一樣)

```Python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1) 

m = 100
x1 = 50 + 30 * np.random.rand(m, 1)
y = 135 + 0.5 * x1 + 3 * np.random.randn(m, 1)
```


### 進行資料前處理
* 和最小平方法不同的地方，為會對x進行標準化(減去平均再除以標準差)
* 從sklearn引進StandardScaler

```Python
from sklearn.preprocessing import StandardScaler

scal = StandardScaler()

# 進行標準化，fit算出平均數標準差等；而transform則是把feature去減平均數再除以標準差
x1_scal = scal.fit_transform(x1)        
x1_scal[:5]       #取前5項

scal.mean_        #平均數
scal.scale_       #標準差
```

##### (執行後結果)              
![image](https://user-images.githubusercontent.com/102600962/182994747-151397f0-c788-4a53-aca6-ad7909ec29eb.png)

### 接著套入隨機梯度下降法模型
* 模型套入Sklearn的SGDRegressor()

```Python
from sklearn.linear_model import SGDRegressor

# 進行參數調整，tol給於10的-3次方，penalty不對模型作限制，eta0為學習速率初始值給予0.1
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=.1) 
# y原本為100x1的矩陣，而ravel轉為1x100的矩陣
sgd_reg.fit(x1_scal,y.ravel()) 

#和最小平方法不一樣的原因是x有經過標準化
sgd_reg.intercept_
sgd_reg.coef_               
```

##### (執行後結果)                
![image](https://user-images.githubusercontent.com/102600962/182995049-e04ca727-5907-4dd5-9afe-5fcadd0414f3.png)

### 新的x值作預測

```Python
# 不能直接帶入，因為模型為經過標準化，所以要將新的值也經過標準化
x1_new = [[50],[80]]  
# 將新的值代入原本scal裡，會取代舊的scal
x1_new_scal = scal.transform(x1_new)  
sgd_reg.predict(x1_new_scal)
```

##### (執行後結果)      
![image](https://user-images.githubusercontent.com/102600962/182995225-dcbb0cca-cd1b-4703-ac72-48437e9b22d8.png)

## 程式碼
```Python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1) 

m = 100
x1 = 50 + 30 * np.random.rand(m, 1)
y = 135 + 0.5 * x1 + 3 * np.random.randn(m, 1)

#-----------資料前處理，Feature 標準化--------------
from sklearn.preprocessing import StandardScaler

scal = StandardScaler()
# 進行標準化，fit算出平均數標準差等，而transform把feature去減平均數再除以標準差
x1_scal = scal.fit_transform(x1) 

x1_scal[:5]

scal.mean_    #平均數
scal.scale_   #標準差

#------------使用隨機梯度下降法；sklearn 的 SGDRegressor()-------------
from sklearn.linear_model import SGDRegressor

# 進行參數調整，tol給於10的-3次方，penalty不對模型作限制，eta0為學習速率初始值給予0.1
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=.1) 
# y原本為100x1的矩陣，而ravel轉為1x100的矩陣
sgd_reg.fit(x1_scal,y.ravel()) 
# 和最小平方法不一樣的原因是x有經過標準化
sgd_reg.intercept_
sgd_reg.coef_               


#------------Predict--------------
# 不能直接帶入，因為模型為經過標準化，所以要將新的值也經過標準化
x1_new = [[50],[80]]                            
# 將新的值代入原本scal裡，會取代舊的scal
x1_new_scal = scal.transform(x1_new)           
sgd_reg.predict(x1_new_scal)
```
